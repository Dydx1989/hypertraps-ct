---
title: "Demonstration of HyperTraPS in R"
output: html_document
---

HyperTraPS (hypercubic transition path sampling) is a family of algorithms for inferring the dynamics of "accumulation" processes. These are processes where a set of binary features are acquired over time.

HyperTraPS will take a set of observed states, described by binary strings recording the presence/absence of each feature. It may optionally take initial states from which these states are reached, and information on the timings associated with each observation. It returns various summaries of which feature are acquired and when, and how these features influence each other.

### Loading the software

If we just want the HyperTraPS code without other dependencies, we only need Rcpp, and can use the following

```{r}
library(Rcpp)
sourceCpp("hypertraps-r.cpp")
```

If we want various helper functions and ggplot functions in addition, use this

```{r}
source("hypertraps.R")
```

### Simple demo 

Here we'll construct a simple synthetic dataset. The `m.1` matrix will store a set of initial states, and the `m.2` matrix will store a set of later observed states. The first row of `m.1`, for example, stores the state 00000, where no features have been acquired. The first row of `m.2` stores 10000, where the first of five features has been acquired.

The times correspond to each observation, so the transition 00000->10000 described by those first rows has an associated time of 0.1 (in whatever units we are working with).

```{r}
m.1 = matrix(rep(c(0,0,0,0,0,
               1,0,0,0,0,
               1,1,0,0,0,
               1,1,1,0,0,
               1,1,1,1,0,
               0,0,0,0,0,
               0,0,0,0,1,
               0,0,0,1,1,
               0,0,1,1,1,
               0,1,1,1,1),5), byrow=TRUE, ncol=5)
m.2 = matrix(rep(c(1,0,0,0,0,
               1,1,0,0,0,
               1,1,1,0,0,
               1,1,1,1,0,
               1,1,1,1,1,
               0,0,0,0,1,
               0,0,0,1,1,
               0,0,1,1,1,
               0,1,1,1,1,
               1,1,1,1,1),5), byrow=TRUE, ncol=5)
times = rep(c(0.1, 0.2, 0.3, 0.4, 0.5), 10)
```

Let's run HyperTraPS with these "before" and "after" observations. By using `times` as both the start and end time arguments, we say that each transition takes precisely that associated time -- we could allow broader time windows to capture uncertainty in timing. Finally, we provide labels for the five individual features involved.

```{r}
my.post = HyperTraPS(m.2, initialstates_arg = m.1, 
                     starttimes_arg = times, endtimes_arg = times,
                     featurenames_arg = c("A", "B", "C", "D", "E")) 
```

That output takes us through the HyperTraPS process. First, the arguments provided to the function call are described, then the algorithm chosen (MH MCMC) and a summary of the input data is given. To estimate runtime, HyperTraPS reports the time taken for a single likelihood calculation, then scales this by the estimated number of calculations required to give a runtime estimate. Here this is only 1.17 seconds, but for chains long enough for satisfactory convergence (and more complicated systems) this may be dramatically longer.

Because we didn't turn off output, HyperTraPS periodically outputs information about the run as it goes. Every 100th step, it outputs: the likelihood of the current parameterisation; the step acceptance rate through the whole run; the step acceptance rate since the last output; the likelihood of the most recent proposal. These can help design efficient MCMC approaches: if the acceptance rate is too low and/or the recent proposal likelihood is much lower than the current, consider a smaller perturbation kernel. It also helps us see when the chain is "burned in" -- when the likelihood fluctuates, rather than consistently increasing, we're probably more stable.

After the MCMC run, the posterior analysis begins, and outputs a few details. These include the size of the posterior sample set being explored, the model being used, and a quick summary of the mean acquisition orderings of each feature. These are really just checks for debugging; not much useful information can be seen from them.

Now -- visualising the results.

*Transition graph plot.* This plot shows a set of sampled routes on the hypercubic space. Each edge is a transition, corresponding to the acquisition of a new feature. The edges are labelled by the feature acquired and the statistics of the time taken (mean +- s.d.). The edge width gives the probability flux through that transition.

```{r}
plotHypercube.sampledgraph2(my.post)
```

*Bubble plot.* This plot summarises the inferred dynamics of features, forgetting specific states. The size of a circle at point x,y gives the probability that feature y is acquired at ordinal time x.

```{r}
plotHypercube.bubbles(my.post)
```

*Timing histograms.* Histograms of acquisition time for each feature, and probability that each feature is not acquired within a given time threshold (here, 20).

```{r}
plotHypercube.timehists(my.post)
```

*Time series plot.* Each step up the vertical axis corresponds to the acquisition of a new feature. The horizontal axis gives the time of the corresponding event, and the colour labels the feature that was acquired.

```{r}
plotHypercube.timeseries(my.post)
```

### Input/output between R data structure and file format

We can write the output of HyperTraPS to a set of files for storage and later retrieval

```{r}
writeHyperinf(my.post, "simpledemo", my.post$L, postlabel = "simpledemo", fulloutput=TRUE)
```

We can retrieve output from files, which may have been previously written as above, or may have come from running HyperTraPS at the command line.

```{r}
my.post.r = readHyperinf("simpledemo", postlabel = "simpledemo", fulloutput=TRUE)
```

### Other functional examples

HyperTraPS allows substantial flexibility in how evolutionary pathways are inferred, how likelihoods are estimated, and other aspects of the approach. 

If we want to sacrifice some accuracy in estimating likelihood for computational speed, we can run an example with fewer walkers sampling pathways (`walkers_arg`)

```{r}
my.post.sparse = HyperTraPS(m.2, initialstates_arg = m.1, 
                            starttimes_arg = times, featurenames_arg = c("A", "B", "C", "D", "E"), 
                            walkers_arg = 2,
                            limited_output_arg = 1)
```

We can ask HyperTraPS to perform stepwise regularisation after fitting a model, pruning extraneous parameters (`regularise_arg`). The regularisation plot shows how the information criterion behaves as we prune back parameters -- the minimum gives us our optimal model.

```{r}
my.post.regularise = HyperTraPS(m.2, initialstates_arg = m.1, regularise_arg = 1,
                                walkers_arg = 20,
                                limited_output_arg = 1)
plotHypercube.regularisation(my.post.regularise)
plotHypercube.summary(my.post.regularise)
```

We can use simulated annealing to get a maximum likelihood estimate rather than a Bayesian picture (`sa_arg`)

```{r}
my.post.sa = HyperTraPS(m.2, initialstates_arg = m.1, sa_arg = 1,
                        limited_output_arg = 1)
plotHypercube.summary(my.post.sa)
```

We can also use "phenotype landscape inference" -- an unbiased sampling method, unlike the (corrected) bias sampling in HyperTraPS (`PLI_arg`). This takes longer but is more flexible with uncertain data

```{r}
my.post.pli = HyperTraPS(m.2, initialstates_arg = m.1, PLI_arg = 1,
                         limited_output_arg = 1)
plotHypercube.summary(my.post.pli)
```

HyperTraPS supports different parameter structures (`model_arg`). By default we use an "L2" parameterisation, where each feature can individually influence the acquisition of every other feature. "L1" has features completely independent; "L0" has all features identical. "L3" allows *pairs* of features to influence each feature's acquisition; "L4" allows *triplets* of features to influence each feature's acquisition. This approach, labelled -1, allows every edge on the hypercube to have its own independent parameters -- corresponding to the most flexible possible picture, where arbitrary sets of features influence other features. We then regularise as above (`regularise_arg`) to remove extraneous parameters

```{r}
my.post.bigmodel.regularise = HyperTraPS(m.2, initialstates_arg = m.1, model_arg = -1,
                                         regularise_arg = 1, walkers_arg = 20,
                                         limited_output_arg = 1)
plotHypercube.regularisation(my.post.bigmodel.regularise)
```

### Scientific examples

A set of short-form examples from past studies -- these should run in a few minutes and give approximations to the original results. In each case we read the observed states from a file (these are present in different formats, so different curation steps are involved in each case), and feature labels from another file. Then we run HyperTraPS and plot some summaries of the output.

1. Ovarian cancer case study: traits are chromosomal aberrations, observations are independent patient samples.

```{r}
cgh.mat = readLines("RawData/ovarian.txt")
cgh.mat = do.call(rbind, lapply(strsplit(cgh.mat, ""), as.numeric))
cgh.names = as.vector(read.table("RawData/ovarian-names.txt", sep=","))[[1]]

my.post.cgh = HyperTraPS(cgh.mat, 
                        length_index_arg = 3, outputinput_arg = 1, 
                        featurenames_arg = cgh.names,
                        limited_output_arg = 1) 
ggarrange(plotHypercube.lik.trace(my.post.cgh), 
          plotHypercube.bubbles(my.post.cgh, reorder=TRUE), 
          plotHypercube.sampledgraph2(my.post.cgh, no.times=TRUE), nrow=3)
plotHypercube.sampledgraph2(my.post.cgh, no.times=TRUE)
```

2. C4 photosynthesis case study: traits are physical/genetic features associated with C4, observations are (incomplete) phylogenetically independent intermediate species.

This case study involves some uncertain data, which are present as "2"s in the source data -- labelling features which may be 0 or 1.

```{r}
c4.mat = as.matrix(read.table("RawData/c4-curated.csv", sep=","))
c4.names = as.vector(read.table("RawData/c4-trait-names.txt", sep=","))[[1]]

my.post.c4 = HyperTraPS(c4.mat, 
                        length_index_arg = 3, 
                        losses_arg = 1,
                        featurenames_arg = c4.names,
                        limited_output_arg = 1) 
plotHypercube.bubbles(my.post.c4, reorder=TRUE)
```

3. Severe malaria disease progression case study: traits are clinical features, observations are (incomplete) independent patient presentations

```{r}
malaria.df = read.csv("RawData/jallow_dataset_binary_with2s.csv")
malaria.mat = as.matrix(malaria.df[,2:ncol(malaria.df)])
malaria.names = as.vector(read.table("RawData/malaria-names.txt", sep=","))[[1]]

my.post.malaria = HyperTraPS(malaria.mat, 
                        length_index_arg = 3,
                        kernel_index_arg = 2,
                        walkers_arg = 20,
                        featurenames_arg = malaria.names,
                        limited_output_arg = 1) 
plotHypercube.bubbles(my.post.malaria, reorder=TRUE, transpose=TRUE)
```

4. Tool use evolution case study: traits are modes of tool use, observations are phylogenetically coupled species observations (phylogeny has been accounted for, giving transition pairs)

```{r}
tools.mat = as.matrix(read.table("RawData/total-observations.txt-trans.txt"))
tools.names = as.vector(read.table("RawData/tools-names.txt"))[[1]]
tools.starts = tools.mat[seq(from=1, to=nrow(tools.mat), by=2),]
tools.ends = tools.mat[seq(from=2, to=nrow(tools.mat), by=2),]

my.post.tools = HyperTraPS(tools.ends, initialstates_arg = tools.starts, 
                           length_index_arg = 3, 
                           featurenames_arg = tools.names,
                           limited_output_arg = 1) 
plotHypercube.bubbles(my.post.tools, reorder=TRUE, transpose=TRUE)
plotHypercube.sampledgraph2(my.post.tools, node.labels = FALSE, max=100, no.times=TRUE) + theme(legend.position = "none")
```

